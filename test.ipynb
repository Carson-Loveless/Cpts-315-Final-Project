{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9619862200047518\n"
     ]
    }
   ],
   "source": [
    "import nltk as nl\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity, treebank, stopwords\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "#import plotly.graph_objects as go\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "stop = set()\n",
    "\n",
    "flag =1\n",
    "inFile = open(\"./stoplist.txt\",\"r\")\n",
    "while flag == 1:\n",
    "    line = inFile.readline()\n",
    "    if line == \"\":\n",
    "        flag = 0\n",
    "    stop.add(line[:-1])\n",
    "inFile.close()\n",
    "#GET THE TRAINING DATA PRE-PROCESSED\n",
    "df = pd.read_csv('big_train.csv', delimiter=',') \n",
    "\n",
    "comments =[]\n",
    "tags={}\n",
    "vocablist={}\n",
    "for i in range(len(df)):\n",
    "    tag = df.iloc[i, 2] or df.iloc[i, 3] or df.iloc[i, 4] or df.iloc[i, 5] or df.iloc[i, 6] or df.iloc[i, 7]\n",
    "    tokens = nl.regexp_tokenize(df.iloc[i, 1], r'[,\\.\\?\\n\\t0123456789 •/\\'><;\\|:·[\\]{}+-=_()!@#$%^&*~`\"]\\s*', gaps=True)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    reduced = set()\n",
    "    for word in tokens:\n",
    "        if word not in stop:\n",
    "            reduced.add(word)\n",
    "            if word not in vocablist.keys():\n",
    "                vocablist[word] = 1\n",
    "            else:\n",
    "                vocablist[word]+=1\n",
    "    un_token = TreebankWordDetokenizer() \n",
    "    comments.append(un_token.detokenize(reduced))\n",
    "    tags[i] = tag\n",
    "\n",
    "\n",
    "\n",
    "count_vector = CountVectorizer()\n",
    "dataframe = count_vector.fit_transform(comments)\n",
    "bag_o_words = dataframe.toarray()#pd.DataFrame(dataframe.toarray(), columns = count_vector.get_feature_names_out())\n",
    "features = count_vector.get_feature_names_out()\n",
    "\n",
    "\n",
    "#Begin creating Naive Bayes\n",
    "X = bag_o_words\n",
    "tags = pd.DataFrame(list(tags.values()))\n",
    "Y = tags[0]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, stratify=Y)\n",
    "nb = MultinomialNB().fit(X,Y)\n",
    "prediction = nb.predict(X_test)\n",
    "print(accuracy_score(y_test, prediction))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# reduced = set()\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# for word in tokens:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#     if word not in stop:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#         reduced.add(word)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m bag \u001b[38;5;241m=\u001b[39m \u001b[43mbag_o_words\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_numpy\u001b[49m()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# bag_matrix = np.empty(0)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# for i, row in bag_o_words.iterrows():\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#     val = df.loc[i, :].values.flatten().tolist()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# custom_testX = bag_test\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# custom_testY = test_tags[0]\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_numpy'"
     ]
    }
   ],
   "source": [
    "    # reduced = set()\n",
    "    # for word in tokens:\n",
    "    #     if word not in stop:\n",
    "    #         reduced.add(word)\n",
    "    bag = bag_o_words.to_numpy()\n",
    "    # bag_matrix = np.empty(0)\n",
    "# for i, row in bag_o_words.iterrows():\n",
    "#     val = df.loc[i, :].values.flatten().tolist()\n",
    "#     bag_matrix=np.append(bag_matrix,val, axis=0)\n",
    "\n",
    "# bag_matrix\n",
    "\n",
    "# count_vector = CountVectorizer()\n",
    "# dataframe = count_vector.fit_transform(comments)\n",
    "# bag_o_words = dataframe.toarray()\n",
    "# words = count_vector.get_feature_names_out()\n",
    "\n",
    "\n",
    "#GET THE TEST DATA PRE-PROCESSED\n",
    "# d = pd.read_csv('test_data.csv', delimiter=',')\n",
    "\n",
    "# test_tags={}\n",
    "# comments2 =[]\n",
    "# count=1\n",
    "# for i in range(len(d)):\n",
    "#     count+=1\n",
    "#     tag = d.iloc[i, 2] or d.iloc[i, 3] or d.iloc[i, 4] or d.iloc[i, 5] or d.iloc[i, 6] or d.iloc[i, 7]\n",
    "#     tokens = nl.regexp_tokenize(d.iloc[i, 1], r'[,\\.\\?\\n\\t0123456789 •/\\'\\|><;:[\\]{}+-·=_()!@#$%^&*~`\"]\\s*', gaps=True)\n",
    "#     tokens = [token.lower() for token in tokens]\n",
    "#     less=set()\n",
    "#     for token in tokens:\n",
    "#         if token in vocablist.keys():\n",
    "#             less.add(token)\n",
    "#             un_token = TreebankWordDetokenizer() \n",
    "#     comments2.append(un_token.detokenize(less))\n",
    "#     test_tags[i] = tag\n",
    "# test_tags[count]=0 \n",
    "# comments2.append(un_token.detokenize(list(vocablist.keys())))\n",
    "\n",
    "# count_vector2 = CountVectorizer()\n",
    "# dataframe2 = count_vector2.fit_transform(comments2)\n",
    "# bag_test = dataframe2.toarray()\n",
    "# features2 = count_vector2.get_feature_names_out()\n",
    "\n",
    "# test_tags = pd.DataFrame(list(test_tags.values()))\n",
    "# custom_testX = bag_test\n",
    "# custom_testY = test_tags[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d426612f250d533e84255222bfb25825e73801a40cc381a7e0ce16c0e7ec383"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
